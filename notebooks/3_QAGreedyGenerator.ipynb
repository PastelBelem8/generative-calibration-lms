{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f349d70",
   "metadata": {},
   "source": [
    "# 3. Scores Generator (A)\n",
    "\n",
    "We have divided this notebook into the following parts:\n",
    "\n",
    "1. Load **matrix**: We load a CSV file with the preprocessed matrix. \n",
    "2. Load **model**: Using hugging face API, we load a pre-trained or a fine-tuned model and apply it to the said matrix to obtain corresponding predictions.\n",
    "\n",
    "3. **Model-specific preprocessing**: We apply model specific fine-tuning that is related with how the models were trained to encode the strings.\n",
    "3. Create **preds**: We create a CSV file with the predictions concerning the model to evaluate.\n",
    "\n",
    "**Note**: We assume that all matrices have a set of `ID_COLS` that uniquely identifies each row. Additionally, for multi-way (or multi-annotated) datasets, we assume a row-wise format, that is, all the necessary data has already been unrolled  along the first dimension. For example, let us consider a __source dataset__ with $200$ examples, where each of them comprises two different annotations. This notebook __assumes that dataset was previously preprocessed__ and is __now unflattened__ totalling $400$ rows (one per example and annotation) when loaded from memory. While this duplicates memory, it avoids having complex pipelines with intrinsic hand-tailored routines for each dataset (i.e., _bye bye spaghetti_ code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3173c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"../outputs\"\n",
    "\n",
    "MODEL_NAME = \"allenai/unifiedqa-t5-small\"\n",
    "#model_name = \"t5-small\"\n",
    "\n",
    "# name of the dataset to preprocess\n",
    "# DATASET_NAME, SPLIT_NAME = \"squad\", \"validation\"\n",
    "DATASET_NAME, SPLIT_NAME = \"newsqa\", \"dev\"\n",
    "# DATASET_NAME, SPLIT_NAME = ('squadshifts', 'new_wiki'), \"test\"\n",
    "# DATASET_NAME, SPLIT_NAME = ('squadshifts', 'nyt'), \"test\"\n",
    "# DATASET_NAME, SPLIT_NAME = ('squadshifts', 'amazon'), \"test\"\n",
    "# DATASET_NAME, SPLIT_NAME = ('squadshifts', 'reddit'), \"test\"\n",
    "# DATASET_NAME, SPLIT_NAME = \"narrativeqa\", \"test_5k_sample_seed_2022\"\n",
    "\n",
    "\n",
    "IS_LOCAL_FS_DATASET = True \\\n",
    "    if (DATASET_NAME in (\"newsqa\", ) or SPLIT_NAME in (\"test_5k_sample_seed_2022\",)) \\\n",
    "    else False\n",
    "\n",
    "if isinstance(DATASET_NAME, tuple):\n",
    "    NORMALIZED_DATASET_NAME = \"\".join(DATASET_NAME)\n",
    "else:\n",
    "    NORMALIZED_DATASET_NAME = DATASET_NAME\n",
    "\n",
    "BASE_FILENAME = f\"{NORMALIZED_DATASET_NAME}_{SPLIT_NAME}\"\n",
    "\n",
    "\n",
    "ROOT_DIR = f\"{OUTPUT_DIR}/results/{NORMALIZED_DATASET_NAME}/{SPLIT_NAME}\"\n",
    "\n",
    "MATRIX_DIR = f\"{ROOT_DIR}/matrix\"\n",
    "MATRIX_FILEPATH = f\"{MATRIX_DIR}/{BASE_FILENAME}_preprocessed.csv\"\n",
    "\n",
    "# Outputs\n",
    "PREDS_DIR = f\"{ROOT_DIR}/preds\"\n",
    "!mkdir -p {PREDS_DIR}\n",
    "\n",
    "SEED = 42\n",
    "# Arguments used to read the files from disk\n",
    "csv_kwargs = {\n",
    "   \"compression\": \"gzip\",\n",
    "   \"encoding\": \"utf-8\",\n",
    "}\n",
    "\n",
    "# ----------------------------------------\n",
    "## Columns names\n",
    "# ----------------------------------------\n",
    "ID_COLS = [\"example_id\", \"answer_id\"]\n",
    "\n",
    "UNIQUE_ID_COL = ID_COLS[0]\n",
    "NON_UNIQUE_ID_COL = ID_COLS[1]\n",
    "print(\"Using\", UNIQUE_ID_COL, \"as the unique column to de-duplicate the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e39fac",
   "metadata": {},
   "source": [
    "## Load matrix \n",
    "\n",
    "This is the preprocessed matrix that will be used by every model when creating predictions. We expect it to  have the following columns:\n",
    "- `ID_COLS: List[str]`, can be one or more set of unique identifier columns.\n",
    "- `TOPIC: str`, optional, provides a high-level categorization of the different examples.\n",
    "\n",
    "- Dataset specific columns, such as `CONTEXT`, `QUESTION`, `ANSWER` for open-book (closed-domain) QA tasks. Amongst these we usually define the `TARGET_LABEL` and the `FEATURES` the ones that will be encoded together for generative purposes.\n",
    "\n",
    "\n",
    "By default we will assume the following columns:\n",
    "- `TARGET_LABEL = 'label'`\n",
    "- `FEATURES = ['question', 'context']`\n",
    "\n",
    "\n",
    "**Note**: ~~May have to reconsider the use of pandas, for larger datasets, since it wont be feasible to hold them in memory. Instead, may consider HuggingFace `datasets` or `pyspark`.~~ Consider [building script](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-or-remote-files) in case more demanding needs arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LABEL = \"label\"\n",
    "FEATURES = [\"question\", \"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "matrix = datasets.load_dataset('csv', data_files=MATRIX_FILEPATH)[\"train\"]\n",
    "print(\"Loaded\", len(matrix), \"datapoints from\", MATRIX_FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd895cf5",
   "metadata": {},
   "source": [
    "### Remove duplicate entries when generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7464e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import drop_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9aca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = drop_duplicates(matrix, UNIQUE_ID_COL)\n",
    "print(\"Remaining\", len(matrix), \"datapoints after dropping duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d440a",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Using HF's API, we load a pre-trained or a fine-tuned model and apply it to the said matrix to obtain corresponding predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import T5Model, UnifiedQAT5Model\n",
    "\n",
    "if \"unified\" in MODEL_NAME:\n",
    "    print(\"Using UnifiedQA:\", MODEL_NAME)\n",
    "    MODEL = UnifiedQAT5Model\n",
    "elif \"t5\" in MODEL_NAME:\n",
    "    print(\"Using T5 model:\", MODEL_NAME)\n",
    "    MODEL = T5Model\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b036ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf_kwargs = {\n",
    "    # Path to directory to store the pretrained models\n",
    "    # (may make ensuing analysis faster)\n",
    "    \"cache_dir\": f\"{OUTPUT_DIR}/model/cache\",\n",
    "    # Specific version of the model to use (defaults to main)\n",
    "    # \"revision\": \"main\",\n",
    "}\n",
    "\n",
    "model_hyperparameters = {\n",
    "    \"padding\": \"max_length\",\n",
    "    \"max_length\": 512,\n",
    "    \n",
    "    \"truncation\": True,\n",
    "    \"add_special_tokens\": True,\n",
    "    \"return_attention_mask\": True,\n",
    "    # All generate-specific kwargs should start with the prefix \"generate_\" \n",
    "    \"generate__max_length\": 100,\n",
    "    \"generate__batch_size\": 500,\n",
    "}\n",
    "\n",
    "model = MODEL(MODEL_NAME, model_hyperparameters, model_hf_kwargs)\n",
    "model.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81cc15",
   "metadata": {},
   "source": [
    "## Generate predictions\n",
    "Using the model and the preprocessed matrix, generate the predictions. \n",
    "The predictions files will contain the following information:\n",
    "\n",
    "Useful resources:\n",
    "- [dataset and Pytorch](https://huggingface.co/docs/datasets/use_dataset.html)\n",
    "- [fine-tuning a pretrained model](https://huggingface.co/course/chapter3/4?fw=pt)\n",
    "- [generator](https://huggingface.co/docs/transformers/v4.16.2/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6933a7fc",
   "metadata": {},
   "source": [
    "### Model-tailored Preprocessing\n",
    "\n",
    "We apply model specific fine-tuning that is related with how the models were trained to encode the strings. We will apply this on a per-batch basis to avoid additional overhead in iterating the datasets. We use the [`datasets.Dataset.set_format`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.set_format) as a more efficient way to cast the necessary columns to pytorch structures. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddf91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_fmt = matrix.map(model._format_row, fn_kwargs={\"features\": FEATURES})\n",
    "matrix_fmt = matrix_fmt.map(lambda examples: model.encode(examples, 'encoded'), batched=True)\n",
    "matrix_fmt.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], output_all_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4634dec",
   "metadata": {},
   "source": [
    "### Creating Greedy Predictions\n",
    "\n",
    "We want to be able to create predictions both for __beam search__ and for __greedy search__. We will focus for now in the case when we have a single return sequence (even though we can have multiple beams or multiple paths explored).\n",
    "\n",
    "A predictions matrix will have the following attributes/columns:\n",
    "- `ID_COLUMNS`: ideally comprised of the unique identifiers you specified in the beginning.\n",
    "- `pred_id`: unique identifier for each example (computed for each instance based on the model_uuid and the generated tokens).\n",
    "- `score_proba`: score associated with the generated sentence. computed as the multiplication of the individual raw_scores. the score is within $[0, 1]$.\n",
    "- `preds`: textual representation of the generated instance\n",
    "- `preds_raw_int`: tokens id \n",
    "- `preds_raw_str`: tokens str\n",
    "- `preds_raw_scores`: scores for each of the tokens, lie in the range $[0, 1]$.\n",
    "- `len`: length of the sentence\n",
    "- `truncated`: whether the sequence was truncated (i.e., actually had the eos token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.predictions import GreedyGenerator\n",
    "from utils_generic import filter_params_by_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47292c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PREFIX = \"generate__\"\n",
    "\n",
    "model_generate_hyperparams = filter_params_by_prefix(model_hyperparameters, GENERATE_PREFIX)\n",
    "model_generate_hyperparams = {param_name[len(GENERATE_PREFIX):]: param_val for param_name, param_val in model_generate_hyperparams.items()}\n",
    "print(\"Generator kwargs:\", model_generate_hyperparams)\n",
    "\n",
    "\n",
    "print(\"Creating **Greedy Generator**\")\n",
    "generator = GreedyGenerator()\n",
    "\n",
    "print(\"Generating...\")\n",
    "batches = generator.generate(\n",
    "    data=matrix_fmt,\n",
    "    id_cols=ID_COLS,\n",
    "    model=model._model,\n",
    "    tokenizer=model._tokenizer,\n",
    "    **model_generate_hyperparams,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26319c06",
   "metadata": {},
   "source": [
    "## Dump prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.output import OutputResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09db3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check (:\n",
    "out_result = OutputResult(\n",
    "    filename=BASE_FILENAME + f\"_{NORMALIZED_DATASET_NAME}_{SPLIT_NAME}\",\n",
    "    output_dir=PREDS_DIR,\n",
    "    out_extension=\"csv.gz\",\n",
    ")\n",
    "\n",
    "print(\"Writing predictions at:\", out_result.filename)\n",
    "out_result.write(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce0eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(out_result.filepath, compression=\"gzip\", encoding=\"utf-8\").isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c138a176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
