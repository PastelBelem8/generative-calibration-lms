{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b18ff90",
   "metadata": {},
   "source": [
    "Back in [January 2022](https://www.notion.so/Generative-QA-datasets-prospection-e12c08fcc1654db1b4d3cb6e1c9ab5b2#c3318815c5214b6ab5e3799a1cf1f408), I started playing around w/ MS Marco dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142a84d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kat/miniconda3/envs/gqa-env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "\n",
    "import datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['tagger', 'parser', 'ner'])\n",
    "\n",
    "\n",
    "def generate_uuid(content, indent: int = 2) -> str:\n",
    "    \"\"\"Deterministic uuid generator of the `content`.\"\"\"\n",
    "    content = json.dumps(content, sort_keys=True, indent=indent).encode(\"utf-8\")\n",
    "    return hashlib.md5(content).hexdigest()\n",
    "\n",
    "# Consider this for evaluation of MS Marco:\n",
    "# https://github.com/microsoft/MSMARCO-Question-Answering/blob/8ecc639c02f697e0b434d0ca04a6528537c20407/Evaluation/eval_exp.py#L22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6585a1",
   "metadata": {},
   "source": [
    "```python\n",
    "{\n",
    "    'answers': array(['No Answer Present.'], dtype=object),\n",
    "    'passages': {\n",
    "        'is_selected': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
    "        'passage_text': [\n",
    "            'Trust to build up to 1.1M sq. ft. mixed-use site on former IKEA parcel to serve. 4,500 Partners employees. SOMERVILLE. – The state’s largest private employer, Partners HealthCare, will be the first. major industry tenant to take up occupancy in Somerville’s new Assembly Row. neighborhood. The nonprofit healthcare provider plans to build up to 1.1. million square feet of new mixed-use office space in several phases on the.',\n",
    "             'RSS Feed: Pictured in the photo, left to right. SOMERVILLE – Partners HealthCare formally broke ground on its new administrative offices in Assembly Square yesterday that will bring approximately 4,500 permanent and 1,500 construction jobs to the 45-acre Assembly Row development.',\n",
    "             'I am thrilled that Partners HealthCare is coming to Assembly Row. The entire area is one of the most vibrant sections of the City and the recent opening of an Orange Line MBTA stop has made it easier to enjoy all that Assembly Row has to offer.',\n",
    "             'A rendering of the new offices for Partners HealthCare proposed for Assembly Square in Somerville. Globe Staff August 27, 2014. Partners HealthCare, one of the state’s largest nonprofits, has agreed to pay Somerville more than 1 million a year when it moves 4,500 employees to new offices in Assembly Square. The payments are in lieu of local property taxes, which nonprofits, such as hospitals and universities, are exempt from paying.',\n",
    "             'The Partners Healthcare building in Assembly Row – just over the line from Everett and Charlestown – will eventually bring close to 4,000 hospital administration workers to the area from 14 locations currently spread out all over Greater Boston – including many workers nearby in the Schrafft’s Building in Charlestown.',\n",
    "             \"Monday, December 01, 2014. PARTNERS HEALTHCARE BREAKS GROUND IN SOMERVILLE. Assembly Row's first major industry tenant bringing 4,500 jobs to new office and retail building; Curtatone, Capuano, Gottleib & other officials mark start of construction.\",\n",
    "             'Partners Building in Assembly Row Scheduled to Open June 13. It’s been going up in the distance for some time. Floor by floor, the glass exterior has risen up to its 13-story height, and the promise of nearly 4,000 workers coming into the area has also been on the horizon.',\n",
    "             'SOMERVILLE - Partners HealthCare formally broke ground on its new administrative offices in Assembly Square yesterday that will bring approximately 4,500 permanent and 1,500 construction jobs to the 45-acre Assembly Row development.',\n",
    "             '“It is a 13-story building,” said Rich Copp of Partners Healthcare. “We’re ready to open the building with the goal of it beginning on June 13. To be clear, it’s going to take at least 18 months for all of the employees to get under one roof. When it is finished, it will be close to 4,000 employees at that location.”.',\n",
    "             \"Partners Care Decisions. Making it easier to get the right care for you. Partners Care Decisions is a program that helps patients make health care decisions that are right for them. It combines their background, doctors' expertise, and data about other similar patients to ultimately help improve their quality of life.\"\n",
    "    ],\n",
    "      'url': [\n",
    "          'http://www.somervillema.gov/news/partners-healthcare-build-ikea-site',\n",
    "         'http://www.partners.org/Newsroom/Press-Releases/Assembly-Square-Office-Ground-Breaking.aspx',\n",
    "         'http://www.somervillema.gov/news/partners-healthcare-breaks-ground-somerville',\n",
    "         'https://www.bostonglobe.com/business/2014/08/26/partners-healthcare-developing-offices-somerville-agrees-pay-city-million-year/QRMaPM21gqypRVe8EDnlhO/story.html',\n",
    "         'http://charlestownbridge.com/2016/06/04/partners-building-in-assembly-row-scheduled-to-open-june-13/',\n",
    "         'http://www.somervillema.gov/news/partners-healthcare-breaks-ground-somerville',\n",
    "         'http://charlestownbridge.com/2016/06/04/partners-building-in-assembly-row-scheduled-to-open-june-13/',\n",
    "         'http://www.somervillema.gov/news/partners-healthcare-breaks-ground-somerville',\n",
    "         'http://charlestownbridge.com/2016/06/04/partners-building-in-assembly-row-scheduled-to-open-june-13/',\n",
    "         'https://www.partners.org/']\n",
    "      },\n",
    " 'query': 'partners healthcare assembly row phone number',\n",
    " 'query_id': 471988,\n",
    " 'query_type': 'NUMERIC',\n",
    " 'wellFormedAnswers': []}\n",
    " ```\n",
    "\n",
    "\n",
    "\n",
    "Let us compute the same information as we did for MOCHA:\n",
    "- `num_passages`\n",
    "- `num_ques_ref_pairs`\n",
    "- `num_instances`\n",
    "- `avg_annots_per_ques_ref_pair`\t\n",
    "- `pct_ref_cont_overlap`\n",
    "- `pct_ref_ques_overlap`\n",
    "- `avg_passage_len`\n",
    "- `avg_question_len`\n",
    "- `avg_reference_len`\n",
    "- `avg_candidate_len`\n",
    "- `avg_candidate_scores`\n",
    "\n",
    "Plus, we also want information about: \n",
    "- `num_no_answer`\n",
    "- `num_well_formed_answers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb6b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of individual passages: for any repeated passage\n",
    "# we'll observe a repeated count\n",
    "num_paragraphs = defaultdict(int)\n",
    "\n",
    "# Number of contexts: for any repeated context\n",
    "# we'll observe a repeated count\n",
    "num_contexts = defaultdict(int)\n",
    "\n",
    "# Number of questions: compute number of questions\n",
    "num_questions = defaultdict(int)\n",
    "\n",
    "# Number of passage-question pairs: \n",
    "# how many times we repeat the same question\n",
    "num_paragraph_question_pairs = defaultdict(int)\n",
    "\n",
    "\n",
    "def build_context(example, sep=\" \\n \"):\n",
    "    passages = example[\"passages\"]\n",
    "    example[\"context\"] = sep.join(passages[\"passage_text\"])\n",
    "        \n",
    "    context_uuid = generate_uuid(example[\"context\"])\n",
    "    num_contexts[context_uuid] += 1.0\n",
    "    \n",
    "    for passage in passages[\"passage_text\"]:\n",
    "        uuid_passage = generate_uuid(passage)\n",
    "        \n",
    "        # Update counts\n",
    "        num_paragraphs[uuid_passage] += 1.0\n",
    "    return example\n",
    "\n",
    "\n",
    "def populate_num_questions(example):        \n",
    "    question_uuid = generate_uuid(example[\"query\"])\n",
    "    num_questions[question_uuid] += 1.0\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "def populate_num_answers(example):\n",
    "    answers = example[\"answers\"]\n",
    "    answers = filter(lambda s: s.lower() != \"no answer present.\", answers)\n",
    "    example[\"num_answers\"] = len(list(answers))\n",
    "    example[\"num_passages_selected\"] = sum(example[\"passages\"][\"is_selected\"])\n",
    "    \n",
    "    example[\"num_reviewed_answers\"] = len(example[\"wellFormedAnswers\"])\n",
    "    return example\n",
    "\n",
    "\n",
    "def num_para_ques_pairs(example):\n",
    "    for passage in example[\"passages\"][\"passage_text\"]:\n",
    "        pair = example[\"query\"] + passage\n",
    "        pair_uuid = generate_uuid(pair)\n",
    "        num_paragraph_question_pairs[pair_uuid] +=1\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "def compute_len_statistics(example, tokenizer=nlp):\n",
    "    example[\"context_len\"]  = len(tokenizer(example[\"context\"]))\n",
    "    example[\"question_len\"] = len(tokenizer(example[\"query\"]))\n",
    "    example[\"paragraphs_len\"] = mean([len(nlp(passage)) for passage in example[\"passages\"][\"passage_text\"]])\n",
    "    \n",
    "    if example[\"num_answers\"] != 0:\n",
    "        example[\"answers_len\"] = mean([len(nlp(answer)) for answer in example[\"answers\"]])\n",
    "    else:\n",
    "        example[\"answers_len\"] = 0.0\n",
    "    \n",
    "    # Compute length of wellFormedAnswers\n",
    "    if example[\"num_reviewed_answers\"] == 0:\n",
    "        example[\"reviewed_answers_len\"] = 0.0\n",
    "    else:\n",
    "        example[\"reviewed_answers_len\"] = float(mean([len(nlp(answer)) for answer in example[\"wellFormedAnswers\"]]))\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "def pct_answers_overlap(example) -> int:\n",
    "    def _overlap(col1, col2):\n",
    "        overlap_counts = [\n",
    "            1 if answer.lower() in example[col1].lower() else 0.0\n",
    "            for answer in example[col2]\n",
    "        ] or [0.0]\n",
    "        example[f\"avg_{col2}_{col1}_overlap\"] = float(round(mean(overlap_counts), 1))\n",
    "\n",
    "    _overlap_with_context = lambda col: _overlap(\"context\", col)\n",
    "    _overlap_with_question = lambda col: _overlap(\"query\", col)\n",
    "    \n",
    "    _overlap_with_context(\"answers\")\n",
    "    _overlap_with_question(\"answers\")\n",
    "    \n",
    "    _overlap_with_context(\"wellFormedAnswers\")\n",
    "    _overlap_with_question(\"wellFormedAnswers\")    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3c0774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset ms_marco (/home/kat/.cache/huggingface/datasets/ms_marco/v2.1/2.1.0/b6a62715fa5219aea5275dd3556601004cd63945cb63e36e022f77bb3cbbca84)\n",
      "Loading cached processed dataset at /home/kat/.cache/huggingface/datasets/ms_marco/v2.1/2.1.0/b6a62715fa5219aea5275dd3556601004cd63945cb63e36e022f77bb3cbbca84/cache-2ad7261edc61f497.arrow\n",
      "Loading cached processed dataset at /home/kat/.cache/huggingface/datasets/ms_marco/v2.1/2.1.0/b6a62715fa5219aea5275dd3556601004cd63945cb63e36e022f77bb3cbbca84/cache-6f6efa6b23ac02f1.arrow\n",
      "Loading cached processed dataset at /home/kat/.cache/huggingface/datasets/ms_marco/v2.1/2.1.0/b6a62715fa5219aea5275dd3556601004cd63945cb63e36e022f77bb3cbbca84/cache-024f6991853740d4.arrow\n",
      "Loading cached processed dataset at /home/kat/.cache/huggingface/datasets/ms_marco/v2.1/2.1.0/b6a62715fa5219aea5275dd3556601004cd63945cb63e36e022f77bb3cbbca84/cache-b2e2732cd78f2069.arrow\n",
      "Loading cached processed dataset at /home/kat/.cache/huggingface/datasets/ms_marco/v2.1/2.1.0/b6a62715fa5219aea5275dd3556601004cd63945cb63e36e022f77bb3cbbca84/cache-202641841ea5ffbd.arrow\n",
      "  0%|                                                                                                                                                                                   | 0/101093 [00:00<?, ?ex/s]/home/kat/miniconda3/envs/gqa-env/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 101093/101093 [1:05:58<00:00, 25.54ex/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [00:03<00:00, 26.82ba/s]\n"
     ]
    }
   ],
   "source": [
    "def get_statistics_for_split(dataset_split, n=None):\n",
    "    raw_dataset = datasets.load_dataset(\"ms_marco\", 'v2.1', split=dataset_split)\n",
    "    \n",
    "    if n:\n",
    "        raw_dataset = raw_dataset.shuffle(seed=42).select(range(n))\n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # \n",
    "    # Results\n",
    "    # \n",
    "    # -------------------------------------------------------------------\n",
    "    statistics = {}\n",
    "\n",
    "    # Create context\n",
    "    raw_dataset = raw_dataset.map(build_context)\n",
    "\n",
    "    # Collects questions\n",
    "    raw_dataset = raw_dataset.map(populate_num_questions)\n",
    "\n",
    "    # Computes the number of answers, abstractive, and unawnswerable answers.\n",
    "    raw_dataset = raw_dataset.map(populate_num_answers)\n",
    "\n",
    "    # Computes the number of paragraph-question pairs\n",
    "    raw_dataset = raw_dataset.map(num_para_ques_pairs)\n",
    "\n",
    "    # Counts\n",
    "    statistics['num_passages'] = len(num_contexts)\n",
    "    statistics['num_paragraphs'] = len(num_paragraphs)\n",
    "    statistics['num_questions'] = len(num_questions)\n",
    "    statistics[\"num_instances\"] = len(raw_dataset)\n",
    "\n",
    "    # Count number of unique paragraph/passage question pairs\n",
    "    statistics['num_paragraph_question_pairs'] = len(num_paragraph_question_pairs)\n",
    "\n",
    "    # Count number of answers\n",
    "    statistics[\"num_answers\"] = sum(raw_dataset[\"num_answers\"])\n",
    "    statistics[\"num_passages_selected\"] = sum(raw_dataset[\"num_passages_selected\"])\n",
    "    statistics[\"num_reviewed_annotations\"] = sum(raw_dataset[\"num_reviewed_answers\"])\n",
    "\n",
    "\n",
    "    # Count average annotations statistics\n",
    "    statistics[\"avg_num_answers\"] = mean(raw_dataset[\"num_answers\"])\n",
    "    statistics[\"avg_num_passages_selected\"] = mean(raw_dataset[\"num_passages_selected\"])\n",
    "\n",
    "    # Compute average length statistics\n",
    "    raw_dataset = raw_dataset.map(pct_answers_overlap)\n",
    "    raw_dataset = raw_dataset.map(compute_len_statistics)\n",
    "    statistics[\"avg_context_len\"] = mean(raw_dataset[\"context_len\"])\n",
    "    statistics[\"avg_paragraph_len\"] = mean(raw_dataset[\"paragraphs_len\"])\n",
    "    statistics[\"avg_question_len\"] = mean(raw_dataset[\"question_len\"])\n",
    "    statistics[\"avg_answers_len\"] = mean(raw_dataset[\"answers_len\"])\n",
    "\n",
    "\n",
    "    # Compute num reviewed answers\n",
    "    subset_reviewed_answers = raw_dataset.filter(lambda example: example[\"num_reviewed_answers\"] != 0)\n",
    "    statistics[\"avg_num_reviewed_answers\"] =  mean(subset_reviewed_answers[\"num_reviewed_answers\"])\n",
    "    statistics[\"avg_reviewed_answers_len\"] = mean(subset_reviewed_answers[\"reviewed_answers_len\"])\n",
    "\n",
    "    # Compute overlapping statistics\n",
    "    statistics[\"pct_answers_cont_overlap\"] = mean(raw_dataset[\"avg_answers_context_overlap\"])\n",
    "    statistics[\"pct_answers_ques_overlap\"] = mean(raw_dataset[\"avg_answers_query_overlap\"])\n",
    "\n",
    "    statistics[\"pct_reviewed_cont_overlap\"] = mean(subset_reviewed_answers[\"avg_wellFormedAnswers_context_overlap\"])\n",
    "    statistics[\"pct_reviewed_ques_overlap\"] = mean(subset_reviewed_answers[\"avg_wellFormedAnswers_query_overlap\"])\n",
    "\n",
    "    return statistics, raw_dataset, subset_reviewed_answers\n",
    "\n",
    "\n",
    "dev_stats, dataset, subset_reviewed_answers = get_statistics_for_split(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1f8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f11b472",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_98483/1587494323.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dev' is not defined"
     ]
    }
   ],
   "source": [
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a861ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df_train = pd.DataFrame(statistics[\"train\"]).T\n",
    "df_train.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d167d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.DataFrame(statistics[\"dev\"]).T\n",
    "df_dev.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072516df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(statistics[\"test\"]).T\n",
    "df_test.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c14847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1147b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
