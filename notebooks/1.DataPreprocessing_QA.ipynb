{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8a6553",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook gathers the preprocessing stages of each dataset for the analysis during out experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf4c2090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Writing matrix at filepath: ../outputs/results/narrativeqa/test_5k_sample_seed_2022/matrix/narrativeqa_test_5k_sample_seed_2022_preprocessed.csv.gz\n",
      "Using id as the unique column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['id', 'answers']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_DIR = \"../datasets\"\n",
    "OUTPUT_DIR = \"../outputs\"\n",
    "\n",
    "# name of the dataset to preprocess\n",
    "# DATASET_NAME, SPLIT_NAME = \"squad\", \"validation\"\n",
    "# DATASET_NAME, SPLIT_NAME = \"newsqa\", \"test\"\n",
    "# DATASET_NAME, SPLIT_NAME = ('squadshifts', 'new_wiki'), \"test\"\n",
    "# DATASET_NAME, SPLIT_NAME = ('squadshifts', 'nyt'), \"test\"\n",
    "# DATASET_NAME, SPLIT_NAME = ('squadshifts', 'amazon'), \"test\"\n",
    "# DATASET_NAME, SPLIT_NAME = ('squadshifts', 'reddit'), \"test\"\n",
    "DATASET_NAME, SPLIT_NAME = \"narrativeqa\", \"test_5k_sample_seed_2022\"\n",
    "\n",
    "IS_LOCAL_FS_DATASET = True \\\n",
    "    if (DATASET_NAME in (\"newsqa\", ) or SPLIT_NAME in (\"test_5k_sample_seed_2022\",)) \\\n",
    "    else False\n",
    "print(IS)\n",
    "if isinstance(DATASET_NAME, tuple):\n",
    "    NORMALIZED_DATASET_NAME = \"\".join(DATASET_NAME)\n",
    "else:\n",
    "    NORMALIZED_DATASET_NAME = DATASET_NAME\n",
    "\n",
    "BASE_FILENAME = f\"{NORMALIZED_DATASET_NAME}_{SPLIT_NAME}\"\n",
    "\n",
    "ROOT_DIR = f\"{OUTPUT_DIR}/results/{NORMALIZED_DATASET_NAME}/{SPLIT_NAME}\"\n",
    "MATRIX_DIR = f\"{ROOT_DIR}/matrix\"\n",
    "!mkdir -p {MATRIX_DIR}\n",
    "\n",
    "MATRIX_FILEPATH = f\"{MATRIX_DIR}/{BASE_FILENAME}_preprocessed.csv.gz\"\n",
    "print(\"Writing matrix at filepath:\", MATRIX_FILEPATH)\n",
    "\n",
    "SEED = 42\n",
    "# Arguments used to read the files from disk\n",
    "csv_kwargs = {\n",
    "   \"compression\": \"gzip\",\n",
    "   \"encoding\": \"utf-8\",\n",
    "}\n",
    "\n",
    "# ----------------------------------------\n",
    "## Columns names\n",
    "# ----------------------------------------\n",
    "UNIQUE_ID_COL = \"id\"\n",
    "print(\"Using\", UNIQUE_ID_COL, \"as the unique column\")\n",
    "\n",
    "QUESTION_COLNAME = \"question\"\n",
    "CONTEXT_COLNAME = \"context\"\n",
    "ANSWER_COLNAME = \"answers\"\n",
    "\n",
    "UUID_FEATURES = [UNIQUE_ID_COL, ANSWER_COLNAME]\n",
    "UUID_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946174d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import load_dataset, unfold_multiple_answers, create_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "175be10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_KWARGS = {\n",
    "    \"dataset\": DATASET_NAME,\n",
    "    \"split\": SPLIT_NAME,\n",
    "    \"local\": IS_LOCAL_FS_DATASET,\n",
    "    \"local_dir\": DATASET_DIR,\n",
    "    \n",
    "    \"fn_kwargs\": {\n",
    "        \"answer_col\": ANSWER_COLNAME,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15779561",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> Loading dataset with arguments: {'dataset': 'narrativeqa', 'split': 'test_5k_sample_seed_2022', 'local': False, 'local_dir': '../datasets', 'fn_kwargs': {'answer_col': 'answers'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset narrative_qa (/home/kat/.cache/huggingface/datasets/narrative_qa/default/0.0.0/daef7ccc51ec258bef464658d11751bb20f033da9b4c219fd84563b3a4af0422)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown split \"test_5k_sample_seed_2022\". Should be one of ['train', 'test', 'validation'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_166158/1553737341.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mload_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLOAD_KWARGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--> Loading dataset with arguments:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mload_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded dataset with\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"examples:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/PhD/generative-calibration-lms/notebooks/utils/datasets.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(dataset, split, local, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mload_fn_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloading_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloading_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mload_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39-pytorch/lib/python3.9/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, script_version, **config_kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m         \u001b[0mkeep_in_memory\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep_in_memory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mis_small_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m     )\n\u001b[0;32m-> 1714\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_verifications\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_verifications\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m     \u001b[0;31m# Rename and cast features to match task schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39-pytorch/lib/python3.9/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36mas_dataset\u001b[0;34m(self, split, run_post_process, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;31m# Create a dataset for each of the given splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         datasets = utils.map_nested(\n\u001b[0m\u001b[1;32m    764\u001b[0m             partial(\n\u001b[1;32m    765\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_single_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39-pytorch/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types, disable_tqdm)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;31m# Singleton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     disable_tqdm = (\n",
      "\u001b[0;32m~/miniconda3/envs/py39-pytorch/lib/python3.9/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_build_single_dataset\u001b[0;34m(self, split, run_post_process, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;31m# Build base dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ds = self._as_dataset(\n\u001b[0m\u001b[1;32m    795\u001b[0m             \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39-pytorch/lib/python3.9/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_as_dataset\u001b[0;34m(self, split, in_memory)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \"\"\"\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         dataset_kwargs = ArrowReader(self._cache_dir, self.info).read(\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0minstructions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39-pytorch/lib/python3.9/site-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, name, instructions, split_infos, in_memory)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \"\"\"\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_file_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_infos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'Instruction \"{instructions}\" corresponds to no data!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py39-pytorch/lib/python3.9/site-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36mget_file_instructions\u001b[0;34m(self, name, instruction, split_infos)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_file_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_infos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;34m\"\"\"Return list of dict {'filename': str, 'skip': int, 'take': int}\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         file_instructions = make_file_instructions(\n\u001b[0m\u001b[1;32m    185\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiletype_suffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filetype_suffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/py39-pytorch/lib/python3.9/site-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36mmake_file_instructions\u001b[0;34m(name, split_infos, instruction, filetype_suffix)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0minstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Create the absolute instruction (per split)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mabsolute_instructions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstruction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_absolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname2len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     return _make_file_instructions_from_absolutes(\n",
      "\u001b[0;32m~/miniconda3/envs/py39-pytorch/lib/python3.9/site-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36mto_absolute\u001b[0;34m(self, name2len)\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0m_AbsoluteInstruction\u001b[0m \u001b[0minstances\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \"\"\"\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_rel_to_abs_instr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_instr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname2len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrel_instr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_relative_instructions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/py39-pytorch/lib/python3.9/site-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0m_AbsoluteInstruction\u001b[0m \u001b[0minstances\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \"\"\"\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_rel_to_abs_instr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_instr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname2len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrel_instr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_relative_instructions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/py39-pytorch/lib/python3.9/site-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36m_rel_to_abs_instr\u001b[0;34m(rel_instr, name2len)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel_instr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname2len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Unknown split \"{split}\". Should be one of {list(name2len)}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname2len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mfrom_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel_instr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown split \"test_5k_sample_seed_2022\". Should be one of ['train', 'test', 'validation']."
     ]
    }
   ],
   "source": [
    "from utils_generic import filter_params, generate_uuid\n",
    "\n",
    "load_kwargs = LOAD_KWARGS\n",
    "print(\"\\n--> Loading dataset with arguments:\", load_kwargs)\n",
    "data = load_dataset(**load_kwargs)\n",
    "print(\"Loaded dataset with\", len(data), \"examples:\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d346ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfold_kwargs = filter_params(LOAD_KWARGS, unfold_multiple_answers)\n",
    "print(\"\\n--> Unfolding (aka flattening) dataset with arguments:\", unfold_kwargs)\n",
    "data = data.map(unfold_multiple_answers, batched=True, **unfold_kwargs)\n",
    "print(\"Resulting dataset has\", len(data), \"examples:\", data)\n",
    "\n",
    "print(\"\\n--> Generate unique identifier using\", UUID_FEATURES)\n",
    "data = create_metadata(data, col=f\"{ANSWER_COLNAME}_id\", features=UUID_FEATURES, **unfold_kwargs)\n",
    "\n",
    "\n",
    "COLS_NAMES = {\n",
    "    \"id\": \"example_id\",\n",
    "    ANSWER_COLNAME: \"labels\",\n",
    "    f\"{ANSWER_COLNAME}_multi_way\": \"multi_way_labels\", \n",
    "}\n",
    "\n",
    "print(\"\\n--> Renaming column names\", COLS_NAMES)\n",
    "data = data.rename_columns(COLS_NAMES)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c034d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"multi_way_labels\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3499e156",
   "metadata": {},
   "source": [
    "### Dump matrix\n",
    "\n",
    "For some reason, we're not being able to store with compression format using the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56285e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Storing matrix at:\", MATRIX_FILEPATH[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\n",
    "    MATRIX_FILEPATH[:-3],\n",
    "    index=False,\n",
    "    **csv_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.read_csv(MATRIX_FILEPATH[:-3]).tail()\n",
    "d.multi_way_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d8b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -latrh {MATRIX_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88df73c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
